{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this question, you will use the Communities and Crime Data Set from the __[UCI repository ](http://archive.ics.uci.edu/ml/datasets/Communities+and+Crime)__.\n",
    "\n",
    "1. This is a real-life data set and as such would not have the nice properties that we expect. Your first job is to make this dataset usable, by filling in all the missing values.\n",
    "    1. Use the sample mean of each column to fill in the missing attribute. Is this is a good choice? Explain why or why not.\n",
    "    2. What else might you use to fill in the missing attributes?\n",
    "    3. If you have a better method, describe it, and use it for filling in the missing data. Does your method provide improvement in performance ? Explain why this method is better.\n",
    "    4. Turn in the completed data set.\n",
    "\n",
    "\n",
    "2. Fit the above data using linear regression. Report the 5-fold cross-validation error: The MSE of the best fit achieved on test data, averaged over 5 different 80-20 splits, along with the parameters learnt for each of the five models.\n",
    "\n",
    "\n",
    "3. Use Ridge-regression on the above data. Repeat the experiment for different values of λ. Report the MSE for each value, on test data, averaged over 5 different 80-20 splits, along with the parameters learnt.\n",
    "    1. Which value of λ gives the best fit ? Show the comparison in a plot.\n",
    "    2. Is it possible to use the information obtained during this experiment for feature selection? If so, explain how?\n",
    "    3. Show the results of the best fit you achieve with a reduced set of features?\n",
    "    4. How different is the performance of the model with reduced features compared to the model using all the features? Comment about the difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_data(dataset, nb_features, modify=True, impute='knn', write=False, k = 5):\n",
    "    # Initializes the desired training set and returns data in a usable form\n",
    "    # Input: \n",
    "    # dataset - the directory where the .dat file is \n",
    "    # nb_features - the number of features in the dataset\n",
    "    # impute - the imputation method to be used\n",
    "    # write - can we write to a file (if no set to False)\n",
    "    # k - k-neighbours to consider when using KNN imputation\n",
    "    # Outputs: \n",
    "    # X - an n x m matrix with n examples and m features (the independent variable in the \n",
    "    #     dataset)\n",
    "    # Y - an n x 1 vector (the dependent variable in the dataset that \n",
    "    # corresponds to X)\n",
    "    \n",
    "    # Open original file with missing attributes and read from csv\n",
    "    dataset_dict = dict()\n",
    "    with open(dataset) as csvfile:\n",
    "        reader = csv.reader(csvfile) \n",
    "        i = 0\n",
    "        for row in reader: # each row is a list\n",
    "            dataset_dict[i] = [(r) for r in row]\n",
    "            i += 1\n",
    "    dataset_length = i\n",
    "   \n",
    "    # If impute is set to 'knn', we modify the dataset using k-nearest neighbours\n",
    "    # imputation.\n",
    "    if impute == 'knn':\n",
    "        csv_dir = os.path.join(os.getcwd(), 'Datasets/communities_knn{}.csv'.format(k))\n",
    "        # Write to a csv, filling in missing attributes using a KNN approach\n",
    "        if write:\n",
    "            # Get dicts with examples with missing attributes, and complete examples\n",
    "            missing_examples = dict()\n",
    "            complete_examples = dict()\n",
    "            for i in range(dataset_length):\n",
    "                if len(dataset_dict[i]) == nb_features:\n",
    "                    if any(\"?\" in attr for attr in dataset_dict[i][5:]):\n",
    "                        missing_examples[i] = dataset_dict[i][5:]\n",
    "                    else:\n",
    "                        complete_examples[i] = dataset_dict[i][5:]\n",
    "        \n",
    "            knn_examples = kn_neighbours(complete_examples, missing_examples, k)\n",
    "            full_examples = dict()\n",
    "            for i in knn_examples:\n",
    "                full_examples[i] = dataset_dict[i][:5] + knn_examples[i]\n",
    "\n",
    "            with open(csv_dir, 'w') as csv_file:\n",
    "                writer = csv.writer(csv_file, dialect='excel')\n",
    "                for key, value in full_examples.items():\n",
    "                    writer.writerow([csv_format(item) for item in value])\n",
    "            \n",
    "        # Read from csv\n",
    "        results = []\n",
    "        with open(csv_dir) as csvfile:\n",
    "            reader = csv.reader(csvfile) \n",
    "            for row in reader: # each row is a list\n",
    "                results.append([float(r) for r in row[5:]])\n",
    "        \n",
    "        results = np.asarray(results)\n",
    "        x_column_lim = results.shape[1] - 1\n",
    "        X = results[..., 0:x_column_lim]\n",
    "        Y = results[..., x_column_lim]\n",
    "      \n",
    "        return X, Y\n",
    "        \n",
    "    else:\n",
    "        csv_dir = os.path.join(os.getcwd(), 'Datasets/communities_mean.csv')\n",
    "        # Now we want to sum all the entries of the lists that can be summed, and take\n",
    "        # their average\n",
    "        sum_list = []\n",
    "        count_list = []\n",
    "        for i in range(nb_features):\n",
    "            sum_list.append(0)\n",
    "            count_list.append(0)\n",
    "        for i in range(dataset_length):\n",
    "            if len(dataset_dict[i]) == nb_features:\n",
    "                sum_list, count_list = sum_lists(dataset_dict[i], sum_list, count_list)\n",
    "                    \n",
    "        # Generate the list of means across columns. The first five features are not predictive (see\n",
    "        # Datasets/communities.names.txt) and so we exclude them from our write.\n",
    "        mean_list = divide_lists(sum_list, count_list)\n",
    "        \n",
    "        \n",
    "        for i in dataset_dict:\n",
    "            for index, item in enumerate(dataset_dict[i]):\n",
    "               # See if the item is a '?'. If it is, then we change lines[index] to be the mean of the \n",
    "                # appropriate feature. Note we ignore the 5 non-predictive features.\n",
    "                if item == '?':\n",
    "                    if index > 4:\n",
    "                        dataset_dict[i][index] = mean_list[index] \n",
    "        \n",
    "        with open(csv_dir, 'w') as csv_file:\n",
    "                writer = csv.writer(csv_file, dialect='excel')\n",
    "                for key, value in dataset_dict.items():\n",
    "                    writer.writerow([csv_format(item) for item in value])\n",
    "        \n",
    "        # Read from csv\n",
    "        results = []\n",
    "        with open(csv_dir) as csvfile:\n",
    "            reader = csv.reader(csvfile) \n",
    "            for row in reader: # each row is a list\n",
    "                results.append([float(r) for r in row[5:]])\n",
    "        \n",
    "        results = np.asarray(results)\n",
    "        x_column_lim = results.shape[1] - 1\n",
    "        X = results[..., 0:x_column_lim]\n",
    "        Y = results[..., x_column_lim]\n",
    "      \n",
    "        return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_lists(x, sum_list, count_list):\n",
    "    # Given a list x, will add x to the list sum_list and update count_list in indices\n",
    "    # where x was some number\n",
    "    for index, a in enumerate(x):\n",
    "        if is_number(a):\n",
    "            sum_list[index] += float(a)\n",
    "            count_list[index] += 1\n",
    "    return sum_list, count_list\n",
    "\n",
    "def subtract_lists(x, y):\n",
    "    # Given two lists x and y, will take the elementwise difference and return it (returns\n",
    "    # a list)\n",
    "    difference = []\n",
    "    for a, b in zip(x, y):\n",
    "        if is_number(a) and is_number(b):\n",
    "            dif = float(a) - float(b)\n",
    "            difference.append(dif)\n",
    "    return difference\n",
    "\n",
    "def divide_lists(sum_list, count_list):\n",
    "    # Given a sum of lists and a list of elementwise counts, returns a list where each \n",
    "    # element is the mean of the elements in the other lists \n",
    "    \n",
    "    # i.e. sum_list = [2,10,4], count_list = [1,5,4]\n",
    "    #   >>  mean_list = [2,2,1]\n",
    "    \n",
    "    mean_list = []\n",
    "    for i in range(len(sum_list)):\n",
    "        mean_list.append(0)\n",
    "    for index, (a, b) in enumerate(zip(sum_list, count_list)):\n",
    "        if b > 0:\n",
    "            mean_list[index] = a / b\n",
    "        else:\n",
    "            mean_list[index] = 0\n",
    "        \n",
    "    return mean_list\n",
    "\n",
    "def is_number(s):\n",
    "    # Given a variable s, returns true if this variable can be converted to a float\n",
    "    try:\n",
    "        float(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    " def kn_neighbours(complete_examples, missing_examples, k):\n",
    "        # For each missing sample, look through all completed samples. \n",
    "        # Find the distance between them and then keep the k closest completed\n",
    "        # samples. Take their weighted mean. Then replace the '?' in the missing examples\n",
    "        # with the corresponding mean. Return the modified dictionary. \n",
    "        # Note this is a fairly computationally expensive approach\n",
    "        for i in missing_examples:\n",
    "            if i % 200 == 0 :\n",
    "                print('Looking at {}-th missing ex.'.format(i))\n",
    "            k_indices = np.ones(k)\n",
    "            k_nearest = np.ones(k) * 1000.\n",
    "            \n",
    "            for j in complete_examples:\n",
    "                k_indices = [x for _,x in sorted(zip(k_nearest, k_indices))]\n",
    "                k_nearest = sorted(k_nearest)\n",
    "                distance = euclidean_distance(missing_examples[i], complete_examples[j])\n",
    "                idx, old_distance = min_gt(k_nearest, distance)\n",
    "                if idx is not None:\n",
    "                    k_nearest[idx] = distance\n",
    "                    k_indices[idx] = j\n",
    "                    \n",
    "            # Take the mean of the k-nearest complete examples\n",
    "            sum = []\n",
    "            mean = []\n",
    "            k_nearest = np.asarray(k_nearest)\n",
    "            inverse_distance_sum = np.sum(1/k_nearest)\n",
    "            for feature in range(len(missing_examples[i])):\n",
    "                sum.append(0)\n",
    "                for idx in range(len(k_nearest)):\n",
    "                    weight = ((1 /k_nearest[idx]) / inverse_distance_sum)\n",
    "                    sum[feature] += weight * float(complete_examples[k_indices[idx]][feature])\n",
    "       \n",
    "            for idx in range(len(missing_examples[i])):\n",
    "                mean.append(sum[idx])\n",
    "            \n",
    "            for index, item in enumerate(missing_examples[i]):\n",
    "                if item == '?':\n",
    "                    missing_examples[i][index] = mean[index]\n",
    "        \n",
    "        set_length = len(complete_examples) + len(missing_examples)\n",
    "        knn_examples = dict()\n",
    "        for i in range(set_length):\n",
    "            if i in complete_examples:\n",
    "                knn_examples[i] = complete_examples[i]\n",
    "            elif i in missing_examples:\n",
    "                knn_examples[i] = missing_examples[i]\n",
    "            else:\n",
    "                print(\"CAN'T FIND KEY\")\n",
    "        return knn_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error(X, Y, W):\n",
    "    # Implements the mean squared error function, i.e.:\n",
    "    # E = 1/N sum from i = 0 to N [X*W^T - Y]^2\n",
    "    # Inputs:\n",
    "    # X - an n x 1 vector (the independent variable)\n",
    "    # W - a degree x 1 vector (the weights that map X to Y_hat)\n",
    "    # Y - an n x 1 vector (the dependent variable in the dataset that \n",
    "    # corresponds to X)\n",
    "    \n",
    "    # Outputs:\n",
    "    # loss - a float loss computed from the MSE equation (above)\n",
    "    \n",
    "    # Map X to Y_hat with the current weight vector\n",
    "    Y_hat = lin_function(X, W)\n",
    "    inner = np.power((Y_hat - Y), 2)\n",
    "    loss = np.sum(inner) / len(X)\n",
    "    return loss\n",
    "\n",
    "def euclidean_distance(X, Y):\n",
    "    # Computes the euclidean distance between two vectors X and Y, i.e.:\n",
    "    # d = sqrt( sum from i = 0 to N [X(i) - Y(i)]^2 )\n",
    "    # Inputs:\n",
    "    # X - an n x 1 vector (a target vector)\n",
    "    # Y - an n x 1 vector (another vector some distance from X). Note Y may\n",
    "    #     have missing elements/arttributes\n",
    "\n",
    "    # Outputs:\n",
    "    # distance - a distance computed from the above equation\n",
    "    \n",
    "    difference = subtract_lists(X, Y)\n",
    "    sum = 0\n",
    "    for item in difference:\n",
    "        sum += item ** 2\n",
    "    distance = np.sqrt(sum)\n",
    "    return distance\n",
    "\n",
    "def lin_function(X, B):\n",
    "    # Returns a linear output Y for X values for coefficients B.\n",
    "    # i.e. Y = B0 + B1 * X1 + B2 * X2 + ... + Bn * Xn\n",
    "    # Inputs:\n",
    "    # X - an n x 1 vector (the independent variable)\n",
    "    # W - a degree x 1 vector (the weights that map X to Y_hat)\n",
    "    # Outputs:\n",
    "    # Y_hat - an n x 1 vector Y_hat[i] = X[i] * W[0] + ... + \n",
    "    # X[i]**degree * W[degree]\n",
    "    \n",
    "    # Outputs:\n",
    "    # loss - a float loss computed from the MSE equation (above)\n",
    "    \n",
    "    # Map X to Y_hat with the current weight vector\n",
    "    Y = np.zeros(X.shape[0]) \n",
    "    for example in range(X.shape[0]):\n",
    "        for feature in range(X.shape[1]):\n",
    "            Y[example] += X[example][feature] * B[feature]\n",
    "    return Y \n",
    "\n",
    "def min_gt(seq, val):\n",
    "    # Return largest item in a sorted seq for which item > val applies.\n",
    "    # None is returned if seq was empty or all items in seq were >= val.\n",
    "    # Inputs:\n",
    "    # seq - a sorted (small to large) list or 1-D numpy array\n",
    "    # val - the value being compared against\n",
    "\n",
    "    idx = len(seq) - 1\n",
    "    while idx >= 0:\n",
    "        if seq[idx] > val:\n",
    "            return idx, seq[idx]\n",
    "        idx -= 1\n",
    "    return None, None\n",
    "\n",
    "def line_fit(X, Y, regularization=0):\n",
    "    # Minimizes the mean squared error of some function Y = X * W^T \n",
    "    # This was derived in class as W = (X^T X)^-1 * X^T * Y\n",
    "    # First we need to make a matrix X where the rows consist of each X \n",
    "    # and the columns consist of each power of X from 0 to \"degree\"\n",
    "    # Inputs:\n",
    "    # X - an n x m vector (the independent variable), with n examples and m features\n",
    "    # Y - an n x 1 vector (the dependent variable), with 1 output for each of the n examples\n",
    "    # regularization - the lambda to be used for regularization. If 0, there is no regularization\n",
    "    #Outputs:\n",
    "    # B - the regressed weights that map X to Y such that the MSE is minimized\n",
    "    \n",
    "    temp1 = np.matmul(X.transpose(), X) \n",
    "    temp1 += (np.identity(X.shape[1]) * regularization) \n",
    "    temp2 = np.matmul(X.transpose(),Y)\n",
    "    B = np.matmul(np.linalg.pinv(temp1), temp2)\n",
    "   \n",
    "    return B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A couple of small helper functions:\n",
    "\n",
    "# csv_format takes an argument and converts it to a float rounded to two decimals if it's a number,\n",
    "# and otherwise returns the original argument\n",
    "\n",
    "# sync_shuffle takes two numpy arrays and shuffles them along axis 0 without losing the relationship\n",
    "# between the two arrays \n",
    "# i.e. sync_shuffle([1,2,3], [5,7,6]) >> ([3,1,2], [6,5,7])\n",
    "\n",
    "def csv_format(n):\n",
    "    if is_number(n):\n",
    "        s = \"%.2f\" % float(n)\n",
    "        n = float(s)\n",
    "        return n\n",
    "    else:\n",
    "        return n\n",
    "\n",
    "def sync_shuffle(a, b):\n",
    "    assert len(a) == len(b)\n",
    "    p = np.random.permutation(len(a))\n",
    "    return a[p], b[p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rw_xval_sets(data_path, features, write=False, method='knn', k = 10, regression = 0, reduced = False, index_list = None):\n",
    "    nb_samples = 5\n",
    "    if write:\n",
    "        if method=='knn':\n",
    "            X, Y = fill_data(data_path, features, write=True, impute=method, k=k)\n",
    "        else:\n",
    "            X, Y = fill_data(data_path, features, impute='mean')\n",
    "        X = np.hstack([np.ones([X.shape[0],1]), X])\n",
    "        \n",
    "        # Now create 5 80:20 splits\n",
    "        partition = Y.shape[0] // 5\n",
    "\n",
    "        for i in range(nb_samples):\n",
    "            csv_dir_train = os.path.join(os.getcwd(), 'Datasets/CandC-train{}.csv'.format(i+1))\n",
    "            csv_dir_test = os.path.join(os.getcwd(), 'Datasets/CandC-test{}.csv'.format(i+1))\n",
    "            X_shuffle, Y_shuffle = sync_shuffle(X, Y)\n",
    "            X_train = X_shuffle[partition:]\n",
    "            Y_train = Y_shuffle[partition:]\n",
    "            Y_train = Y_train.reshape((Y_train.shape[0], 1))\n",
    "            train = np.append(X_train, Y_train, axis=1)\n",
    "  \n",
    "            X_test = X_shuffle[:partition]\n",
    "            Y_test = Y_shuffle[:partition]\n",
    "            Y_test = Y_test.reshape((Y_test.shape[0], 1))\n",
    "            test = np.append(X_test, Y_test, axis=1)\n",
    "            np.savetxt(csv_dir_train, train, delimiter=\",\")\n",
    "            np.savetxt(csv_dir_test, test, delimiter=\",\")\n",
    "    \n",
    "    test_loss_list = np.zeros(nb_samples)\n",
    "    train_loss_list = np.zeros(nb_samples)\n",
    "    B = []\n",
    "    for i in range(nb_samples):\n",
    "        # Read from csv\n",
    "        train_data = []\n",
    "        test_data = []\n",
    "        csv_dir_train = os.path.join(os.getcwd(), 'Datasets/CandC-train{}.csv'.format(i + 1))\n",
    "        csv_dir_test = os.path.join(os.getcwd(), 'Datasets/CandC-test{}.csv'.format(i + 1))\n",
    "        with open(csv_dir_train) as csvfile:\n",
    "            reader = csv.reader(csvfile) \n",
    "            for row in reader: \n",
    "                train_data.append([float(r) for r in row])\n",
    "        \n",
    "            train_data = np.asarray(train_data)\n",
    "            x_column_lim = train_data.shape[1] - 1\n",
    "            X_train = train_data[..., 0:x_column_lim]\n",
    "            Y_train = train_data[..., x_column_lim]\n",
    "     \n",
    "        with open(csv_dir_test) as csvfile:\n",
    "            reader = csv.reader(csvfile) \n",
    "            for row in reader: \n",
    "                test_data.append([float(r) for r in row])\n",
    "        \n",
    "            test_data = np.asarray(test_data)\n",
    "            x_column_lim = test_data.shape[1] - 1\n",
    "            X_test = test_data[..., 0:x_column_lim]\n",
    "            Y_test = test_data[..., x_column_lim]\n",
    "\n",
    "        if not reduced:\n",
    "            B.append(line_fit(X_train, Y_train, regression))\n",
    "            train_loss = mean_squared_error(X_train, Y_train, B[i])\n",
    "            test_loss = mean_squared_error(X_test, Y_test, B[i])\n",
    "            train_loss_list[i] = train_loss\n",
    "            test_loss_list[i] = test_loss\n",
    "            if not regression:\n",
    "                weight_dir = os.path.join(os.getcwd(), 'Assignment1_260601793_4_2_model{}.txt'.format(i+1))\n",
    "                np.savetxt(weight_dir, B[i], delimiter=\",\")\n",
    "        else:\n",
    "            B.append(line_fit(X_train[:, index_list], Y_train, regression))\n",
    "            train_loss = mean_squared_error(X_train[:, index_list], Y_train, B[i])\n",
    "            test_loss = mean_squared_error(X_test[:, index_list], Y_test, B[i])\n",
    "            train_loss_list[i] = train_loss\n",
    "            test_loss_list[i] = test_loss\n",
    "          \n",
    "    return np.mean(train_loss_list), np.mean(test_loss_list), B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss :  0.015409158249706905\n",
      "Average test loss :  0.01851490941397687\n"
     ]
    }
   ],
   "source": [
    "##################################################################\n",
    "#  REGRESSION (without regularization)\n",
    "##################################################################\n",
    "\n",
    "data_path = os.path.join(os.getcwd(), 'Datasets/communities.csv')\n",
    "features = 128\n",
    "train_loss, test_loss, B = rw_xval_sets(data_path, features, write =False, method='knn', k = 10)\n",
    "print(\"Average training loss : \", train_loss)\n",
    "print(\"Average test loss : \", test_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min loss : 0.018125408568300444\n",
      "Best lambda : 3.282828282828283\n"
     ]
    }
   ],
   "source": [
    "##################################################################\n",
    "#  REGRESSION (with regularization)\n",
    "##################################################################\n",
    "# Explore varying regularization coefficients and get optimum\n",
    "lambdas = np.linspace(0,5, 100)\n",
    "test_loss_list = []\n",
    "\n",
    "for L in lambdas:\n",
    "    train_list, test_loss, B = rw_xval_sets(data_path, features, write =False, regression=L)\n",
    "    test_loss_list.append(test_loss)\n",
    "\n",
    "# Plot regularization coeffient vs MSE\n",
    "plt.plot(lambdas, test_loss_list, 'r')\n",
    "plt.xlabel('Regularization Coefficient')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.ylim([0.0175, 0.0190])\n",
    "plt.title('Test Error as Regularization Coefficient Varies')\n",
    "plt.tight_layout()\n",
    "plt.savefig('polynomial_fit_train&valid_regularization.png')\n",
    "plt.show()\n",
    "\n",
    "min_index = test_loss_list.index(min(test_loss_list))\n",
    "opt_lambda = lambdas[min_index]\n",
    "print(\"Min loss :\", min(test_loss_list))\n",
    "print(\"Best lambda :\", opt_lambda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full feature set train loss : 0.01643227502842467\n",
      "Full feature set test loss :  0.018125408568300444\n",
      "Reduced feature set train loss (filter = 0.04): 0.01691186009578547\n",
      "Reduced feature set test loss (filter = 0.04): 0.017470644766301748\n"
     ]
    }
   ],
   "source": [
    "##################################################################\n",
    "#  REGRESSION (with reduced features)\n",
    "##################################################################\n",
    "\n",
    "# Now test the model with optimal lambda and save weights\n",
    "train_loss, test_loss, B = rw_xval_sets(data_path, features, write=False, regression=opt_lambda)\n",
    "B = np.asarray(B)\n",
    "for i in range(B.shape[0]):\n",
    "    weight_dir = os.path.join(os.getcwd(), 'Assignment1_260601793_4_3_model{}.txt'.format(i+1))\n",
    "    np.savetxt(weight_dir, B[i], delimiter=\",\")\n",
    "mean_B = np.mean(B, axis=0)\n",
    "print(\"Full feature set train loss :\", train_loss)\n",
    "print(\"Full feature set test loss : \", test_loss)\n",
    "\n",
    "      \n",
    "# Reduce features by getting rid of small weights (less than 'filter')\n",
    "\n",
    "filter = 0.04\n",
    "\n",
    "index_list = []\n",
    "for index, weight in enumerate(mean_B):\n",
    "    if np.abs(weight) >= filter:\n",
    "        index_list.append(index)\n",
    "index_list = np.asarray(index_list)\n",
    "train_loss, test_loss, B = rw_xval_sets(data_path, features, write=False, regression=opt_lambda, reduced=True, index_list=index_list)\n",
    "print(\"Reduced feature set train loss (filter = {}):\".format(filter), train_loss)\n",
    "print(\"Reduced feature set test loss (filter = {}):\".format(filter), test_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
