{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) Binary Bag of Words (Yelp Dataset)\n",
    "---------------------------------\n",
    "For this question, we will focus on the yelp dataset with binary bag-of-words (BBoW) representation. We will use the F1-measure as the evaluation metric.\n",
    "As a baseline, we use a random classifier and a majority classifier. Then we train Naive Bayes (Bernoulli), Decision Tree and Linear SVM. We tune hyperparameters with the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import os\n",
    "import pandas as pd\n",
    "import string\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yelp_load():\n",
    "    # Load dataset split (loads numpy arrays in case they aren't loaded)\n",
    "    train_dir = os.path.join(os.getcwd(), 'hwk3_datasets/yelp-train.txt')\n",
    "    val_dir = os.path.join(os.getcwd(), 'hwk3_datasets/yelp-valid.txt')\n",
    "    test_dir = os.path.join(os.getcwd(), 'hwk3_datasets/yelp-test.txt')\n",
    "\n",
    "    train_data = pd.read_csv(train_dir, sep='\\t', names=['review', 'score'], header = None)\n",
    "    val_data = pd.read_csv(val_dir, sep='\\t', names=['review', 'score'], header = None)\n",
    "    test_data = pd.read_csv(test_dir, sep='\\t', names=['review', 'score'], header = None)\n",
    "\n",
    "    # Now, remove punctuation and capital letters - we want to keep only word\n",
    "    # characteres (letters and numbers) so [^\\w\\s]\n",
    "    train_data['review'] = train_data['review'].str.replace(r'[^\\w\\s]+', '')\n",
    "    train_data['review'] = train_data['review'].str.lower()\n",
    "    val_data['review'] = val_data['review'].str.replace(r'[^\\w\\s]+', '')\n",
    "    val_data['review'] = val_data['review'].str.lower()\n",
    "    test_data['review'] = test_data['review'].str.replace(r'[^\\w\\s]+', '')\n",
    "    test_data['review'] = test_data['review'].str.lower()\n",
    "\n",
    "    # Will take the 10000 most frequent words\n",
    "    vectorizer = CountVectorizer(max_features=10000)\n",
    "    vectorizer.fit(train_data['review'])\n",
    "    train_vectors = vectorizer.transform(train_data['review'])\n",
    "    test_vectors = vectorizer.transform(test_data['review'])\n",
    "    val_vectors = vectorizer.transform(val_data['review'])\n",
    "    words = vectorizer.get_feature_names()\n",
    "    train_matrix = np.asarray(train_vectors)\n",
    "    frequency_vector = np.asarray(train_vectors.sum(axis=0)).reshape(10000,)\n",
    "    frequency_list = frequency_vector.tolist()\n",
    "\n",
    "    index_list = range(10000)\n",
    "    frequency_list, index_list, words = zip(*sorted(zip(frequency_list, index_list, words), reverse=True))\n",
    "\n",
    "    # Get the vocabulary. \n",
    "    vocabulary = vectorizer.vocabulary_\n",
    "\n",
    "    with open(\"yelp-vocab.txt\",'w') as vocab_file:\n",
    "        for i in range(10000):\n",
    "            vocab_file.write(\"{0:<12}\\t{1:>5}\\t{2:>8}\\n\".format(words[i], index_list[i], frequency_list[i] ))\n",
    "\n",
    "    # Build train file\n",
    "    nb_examples_train = 7000\n",
    "    nb_examples_val = 1000\n",
    "    nb_examples_test = 2000\n",
    "    nb_features = 10000\n",
    "    example = 0\n",
    "    # Set up BBoW with 1 if example has word in index, 0 if not\n",
    "    BBOW_trainx = np.zeros((nb_examples_train, nb_features))\n",
    "    BBOW_trainy = np.zeros((nb_examples_train,))\n",
    "    # Set up FBoW with word_count/total_count if example has word in index, 0 if not\n",
    "    FBOW_trainx = np.zeros((nb_examples_train, nb_features))\n",
    "    FBOW_trainy = np.zeros((nb_examples_train,))\n",
    "    with open(\"yelp-train.txt\",'w') as train_file:\n",
    "        for review in train_data['review']:\n",
    "            occurances = 0\n",
    "            words = review.split()\n",
    "            paragraph = \"\"\n",
    "            for word in words:\n",
    "                index = vocabulary.get(word)\n",
    "                if index is not None:\n",
    "                    paragraph += str(index)+ \" \"\n",
    "                    BBOW_trainx[example, index] = 1\n",
    "                    FBOW_trainx[example, index] += 1\n",
    "                    occurances += 1\n",
    "            BBOW_trainy[example] = train_data['score'][example]\n",
    "            FBOW_trainy[example] = train_data['score'][example]\n",
    "            train_file.write(\"{}\\t{}\\n\".format(paragraph, train_data['score'][example]))\n",
    "            if occurances != 0:\n",
    "                FBOW_trainx[example] /= occurances\n",
    "            example += 1\n",
    "    np.savetxt(\"inputs/yelp-train-bbow_x.txt\", BBOW_trainx, delimiter=\",\", fmt='%d')\n",
    "    np.savetxt(\"inputs/yelp-train-bbow_y.txt\", BBOW_trainy, delimiter=\",\", fmt='%d')\n",
    "    np.savetxt(\"inputs/yelp-train-fbow_x.txt\", FBOW_trainx, delimiter=\",\", fmt='%1.5f')\n",
    "    np.savetxt(\"inputs/yelp-train-fbow_y.txt\", FBOW_trainy, delimiter=\",\", fmt='%d')\n",
    "\n",
    "    # Build validation file\n",
    "    BBOW_valx = np.zeros((nb_examples_val, nb_features))\n",
    "    BBOW_valy = np.zeros((nb_examples_val,))\n",
    "    FBOW_valx = np.zeros((nb_examples_val, nb_features))\n",
    "    FBOW_valy = np.zeros((nb_examples_val,))\n",
    "    example = 0\n",
    "    with open(\"yelp-val.txt\",'w') as val_file:\n",
    "        for review in val_data['review']:\n",
    "            occurances = 0\n",
    "            words = review.split()\n",
    "            paragraph = \"\"\n",
    "            for word in words:\n",
    "                index = vocabulary.get(word)           \n",
    "                if index is not None:\n",
    "                    paragraph += str(index)+ \" \"\n",
    "                    BBOW_valx[example, index] = 1\n",
    "                    FBOW_valx[example, index] += 1\n",
    "                    occurances += 1\n",
    "            BBOW_valy[example] = val_data['score'][example]\n",
    "            FBOW_valy[example] = val_data['score'][example]\n",
    "            val_file.write(\"{}\\t{}\\n\".format(paragraph, val_data['score'][example]))\n",
    "            if occurances != 0:\n",
    "                FBOW_valx[example] /= occurances\n",
    "            example += 1\n",
    "    np.savetxt(\"inputs/yelp-val-bbow_x.txt\", BBOW_valx, delimiter=\",\", fmt='%d')\n",
    "    np.savetxt(\"inputs/yelp-val-bbow_y.txt\", BBOW_valy, delimiter=\",\", fmt='%d')\n",
    "    np.savetxt(\"inputs/yelp-val-fbow_x.txt\", FBOW_valx, delimiter=\",\", fmt='%1.5f')\n",
    "    np.savetxt(\"inputs/yelp-val-fbow_y.txt\", FBOW_valy, delimiter=\",\", fmt='%d')\n",
    "\n",
    "    # Build test file\n",
    "    BBOW_testx = np.zeros((nb_examples_test, nb_features))\n",
    "    BBOW_testy = np.zeros((nb_examples_test,))\n",
    "    FBOW_testx = np.zeros((nb_examples_test, nb_features))\n",
    "    FBOW_testy = np.zeros((nb_examples_test,))\n",
    "    example = 0\n",
    "    with open(\"yelp-test.txt\",'w') as test_file:\n",
    "        for review in test_data['review']:\n",
    "            occurances = 0\n",
    "            words = review.split()\n",
    "            paragraph = \"\"\n",
    "            for word in words:\n",
    "                index = vocabulary.get(word)           \n",
    "                if index is not None:\n",
    "                    paragraph += str(index)+ \" \"\n",
    "                    BBOW_testx[example, index] = 1\n",
    "                    FBOW_testx[example, index] += 1\n",
    "                    occurances += 1\n",
    "            BBOW_testy[example] = test_data['score'][example]\n",
    "            FBOW_testy[example] = test_data['score'][example]\n",
    "            test_file.write(\"{}\\t{}\\n\".format(paragraph, test_data['score'][example]))\n",
    "            if occurances != 0:\n",
    "                FBOW_testx[example] /= occurances\n",
    "            example += 1\n",
    "    np.savetxt(\"inputs/yelp-test-bbow_x.txt\", BBOW_testx, delimiter=\",\", fmt='%d')\n",
    "    np.savetxt(\"inputs/yelp-test-bbow_y.txt\", BBOW_testy, delimiter=\",\", fmt='%d')\n",
    "    np.savetxt(\"inputs/yelp-test-fbow_x.txt\", FBOW_testx, delimiter=\",\", fmt='%1.5f')\n",
    "    np.savetxt(\"inputs/yelp-test-fbow_y.txt\", FBOW_testy, delimiter=\",\", fmt='%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input files already exist\n"
     ]
    }
   ],
   "source": [
    "# Load up arrays\n",
    "yelp_train_bbow_x = 'inputs/yelp-train-bbow_x.txt'\n",
    "yelp_train_bbow_y ='inputs/yelp-train-bbow_y.txt'\n",
    "yelp_val_bbow_x ='inputs/yelp-val-bbow_x.txt'\n",
    "yelp_val_bbow_y ='inputs/yelp-val-bbow_y.txt'\n",
    "yelp_test_bbow_x ='inputs/yelp-test-bbow_x.txt'\n",
    "yelp_test_bbow_y ='inputs/yelp-test-bbow_y.txt'\n",
    "if os.path.isfile(yelp_train_bbow_x) and os.path.isfile(yelp_val_bbow_x) and os.path.isfile(yelp_test_bbow_x):\n",
    "    print(\"Input files already exist\")\n",
    "    train_x = np.loadtxt(yelp_train_bbow_x, delimiter=',')\n",
    "    train_y = np.loadtxt(yelp_train_bbow_y, delimiter=',')\n",
    "    val_x = np.loadtxt(yelp_val_bbow_x, delimiter=',')\n",
    "    val_y = np.loadtxt(yelp_val_bbow_y, delimiter=',')\n",
    "    test_x = np.loadtxt(yelp_test_bbow_x, delimiter=',')\n",
    "    test_y = np.loadtxt(yelp_test_bbow_y, delimiter=',')\n",
    "else:\n",
    "    print(\"--------Creating input files (might take a while)---------------\")\n",
    "    if not os.path.exists('inputs'):\n",
    "        os.makedirs('inputs')\n",
    "    yelp_load()\n",
    "    print(\"Loading training set...\")\n",
    "    train_x = np.loadtxt(yelp_train_bbow_x, delimiter=',')\n",
    "    train_y = np.loadtxt(yelp_train_bbow_y, delimiter=',')\n",
    "    print(\"Loading validation set...\")\n",
    "    val_x = np.loadtxt(yelp_val_bbow_x, delimiter=',')\n",
    "    val_y = np.loadtxt(yelp_val_bbow_y, delimiter=',')\n",
    "    print(\"Loading test set...\")\n",
    "    test_x = np.loadtxt(yelp_test_bbow_x, delimiter=',')\n",
    "    test_y = np.loadtxt(yelp_test_bbow_y, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_classifier(classifier): \n",
    "    train_yh =classifier.predict(train_x)\n",
    "    val_yh = classifier.predict(val_x)\n",
    "    test_yh = classifier.predict(test_x)\n",
    "    \n",
    "    # Check performance\n",
    "    train_ascore = accuracy_score(train_y, train_yh)\n",
    "    val_ascore = accuracy_score(val_y, val_yh)\n",
    "    test_ascore = accuracy_score(test_y, test_yh)\n",
    "   \n",
    "    val_f1 = f1_score(val_y, val_yh, average='micro')\n",
    "    train_f1 = f1_score(train_y, train_yh, average='micro')\n",
    "    test_f1 = f1_score(test_y, test_yh, average='micro')\n",
    "    \n",
    "    print(\"Train Acc: {}\".format(train_ascore))\n",
    "    print(\"Val Acc: {}\".format(val_ascore))\n",
    "    print(\"Test Acc: {}\".format(test_ascore))\n",
    "    \n",
    "    print(\"Train F1: {}\".format(train_f1))\n",
    "    print(\"Val F1: {}\".format(val_f1))\n",
    "    print(\"Test F1: {}\".format(test_f1))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter Tuning\n",
    "---------------------------------\n",
    "We'll try a grid search over several parameters. The values are refined \n",
    "in the neighbourhood of where performance is best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------Random Classifier ----------------\n",
      "Train Acc: 0.2017142857142857\n",
      "Val Acc: 0.195\n",
      "Test Acc: 0.1915\n",
      "Train F1: 0.2017142857142857\n",
      "Val F1: 0.195\n",
      "Test F1: 0.1915\n",
      "\n",
      "\n",
      "--------Majority Class Classifier ----------------\n",
      "Train Acc: 0.3525714285714286\n",
      "Val Acc: 0.356\n",
      "Test Acc: 0.351\n",
      "Train F1: 0.3525714285714286\n",
      "Val F1: 0.356\n",
      "Test F1: 0.351\n",
      "\n",
      "\n",
      "-------- Naive Bayes Classifier ----------------\n",
      "Hyperparameter tuning...\n",
      "Optimum hyper-parameters:  {'alpha': 0.045}\n",
      "\n",
      "Evaluating...\n",
      "Train Acc: 0.7252857142857143\n",
      "Val Acc: 0.418\n",
      "Test Acc: 0.434\n",
      "Train F1: 0.7252857142857143\n",
      "Val F1: 0.418\n",
      "Test F1: 0.434\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('--------Random Classifier ----------------')\n",
    "random = DummyClassifier(strategy='uniform', random_state=1) \n",
    "random.fit(train_x, train_y)\n",
    "eval_classifier(random)\n",
    "\n",
    "print('--------Majority Class Classifier ----------------')\n",
    "majority = DummyClassifier(strategy='most_frequent')\n",
    "majority.fit(train_x, train_y)\n",
    "eval_classifier(majority)\n",
    "\n",
    "print('-------- Naive Bayes Classifier ----------------')\n",
    "# Here we setup a training validation combined array for cross \n",
    "# validation\n",
    "cv_x = np.append(train_x, val_x, axis = 0)\n",
    "cv_y = np.append(train_y, val_y, axis = 0)\n",
    "\n",
    "print('Hyperparameter tuning...')\n",
    "# Tuning the smoothing parameter 'alpha' of the error term. We locate it on the\n",
    "# range (0.04, 0.05) and find the optimum at 0.045\n",
    "bayes = BernoulliNB()\n",
    "alpha_space = np.linspace(0.042, 0.048, num=5)\n",
    "alpha_list = [a for a in alpha_space]\n",
    "parameters={'alpha' : alpha_list}\n",
    "classifier = GridSearchCV(bayes, parameters, cv = 5, scoring='f1_micro')\n",
    "classifier.fit(cv_x, cv_y)\n",
    "print(\"Optimum hyper-parameters: \", classifier.best_params_)\n",
    "\n",
    "# Now fit model with optimum hyperparameters\n",
    "print('\\nEvaluating...')\n",
    "opt_alpha = classifier.best_params_['alpha']\n",
    "classifier = BernoulliNB(alpha=opt_alpha)\n",
    "classifier.fit(train_x, train_y)\n",
    "eval_classifier(classifier)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------- Decision Tree Classifier ----------------\n",
      "Hyperparameter tuning...\n",
      "{'max_depth': 9}\n",
      "\n",
      "Evaluating...\n",
      "Train Acc: 0.5062857142857143\n",
      "Val Acc: 0.397\n",
      "Test Acc: 0.401\n",
      "Train F1: 0.5062857142857143\n",
      "Val F1: 0.3970000000000001\n",
      "Test F1: 0.401\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('\\n-------- Decision Tree Classifier ----------------')\n",
    "print('Hyperparameter tuning...')\n",
    "# Tuning the depth (other decision tree hyperparameters not seen in class\n",
    "# will not be considered here)\n",
    "max_depths = [8, 9, 10] \n",
    "\n",
    "tree = DecisionTreeClassifier(random_state=1)\n",
    "parameters={'max_depth' : max_depths}\n",
    "classifier = GridSearchCV(tree, parameters, cv = 5, scoring='f1_micro')\n",
    "classifier.fit(cv_x, cv_y)\n",
    "print(classifier.best_params_)\n",
    "\n",
    "# Now train on optimum hyperparameters\n",
    "print('\\nEvaluating...')\n",
    "opt_parameters = classifier.best_params_\n",
    "classifier = DecisionTreeClassifier(random_state=1,max_depth=opt_parameters['max_depth'])\n",
    "                                  \n",
    "classifier.fit(train_x, train_y)\n",
    "eval_classifier(classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Linear SVM Classifier ----------------\n",
      "Hyperparameter tuning...\n",
      "Optimum hyper-parameters:  {'C': 0.0045}\n",
      "\n",
      "Evaluating...\n",
      "Train Acc: 0.7741428571428571\n",
      "Val Acc: 0.501\n",
      "Test Acc: 0.512\n",
      "Train F1: 0.7741428571428571\n",
      "Val F1: 0.501\n",
      "Test F1: 0.512\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('-------- Linear SVM Classifier ----------------')\n",
    "print('Hyperparameter tuning...')\n",
    "# Tuning the penalty parameter 'C' of the error term. We locate it on the\n",
    "# range (0.004, 0.005) and find the optimum at 0.0045\n",
    "parameters={'C': [0.0044, 0.0045, 0.0046]}\n",
    "\n",
    "svm_clf = svm.LinearSVC(random_state=1, max_iter=5000)\n",
    "classifier = GridSearchCV(svm_clf, parameters, cv = 5, scoring='f1_micro')\n",
    "classifier.fit(cv_x, cv_y)\n",
    "print(\"Optimum hyper-parameters: \", classifier.best_params_)\n",
    "\n",
    "# Now train on optimum hyperparameters\n",
    "opt_C = classifier.best_params_['C']\n",
    "print('\\nEvaluating...')\n",
    "classifier = svm.LinearSVC(random_state=1, C=opt_C, max_iter=5000)\n",
    "classifier.fit(train_x, train_y)\n",
    "eval_classifier(classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
