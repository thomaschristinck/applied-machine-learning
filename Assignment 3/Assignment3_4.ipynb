{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(4) Binary Bag of Words (IMDB Dataset)\n",
    "------------\n",
    "For this question, we will focus on the IMDB dataset with binary bag-of-words (BBoW) representation. We will use the F1-measure as the evaluation metric. As a baseline, we use a random classifier. Then we train Naive Bayes (Bernoulli), Decision Tree and Linear SVM. We tune hyperparameters with the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import os\n",
    "import pandas as pd\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imdb_load():\n",
    "        # Load dataset split (loads numpy arrays in case they aren't loaded)\n",
    "        train_dir = os.path.join(os.getcwd(), 'hwk3_datasets/IMDB-train.txt')\n",
    "        val_dir = os.path.join(os.getcwd(), 'hwk3_datasets/IMDB-valid.txt')\n",
    "        test_dir = os.path.join(os.getcwd(), 'hwk3_datasets/IMDB-test.txt')\n",
    "\n",
    "        train_data = pd.read_csv(train_dir, sep='\\t', names=['review', 'score'], header = None)\n",
    "        val_data = pd.read_csv(val_dir, sep='\\t', names=['review', 'score'], header = None)\n",
    "        test_data = pd.read_csv(test_dir, sep='\\t', names=['review', 'score'], header = None)\n",
    "\n",
    "        # Now, remove punctuation and capital letters - we want to keep only word\n",
    "        # characteres (letters and numbers) so [^\\w\\s]\n",
    "        # Note we also want to get rid of '<br />', which is kind of a special case\n",
    "        train_data['review'] = train_data['review'].str.replace('<br />', '')\n",
    "        train_data['review'] = train_data['review'].str.replace(r'[^\\w\\s]+', '')\n",
    "        train_data['review'] = train_data['review'].str.lower()\n",
    "        val_data['review'] = val_data['review'].str.replace('<br />', '')\n",
    "        val_data['review'] = val_data['review'].str.replace(r'[^\\w\\s]+', '')\n",
    "        val_data['review'] = val_data['review'].str.lower()\n",
    "        test_data['review'] = test_data['review'].str.replace('<br />', '')\n",
    "        test_data['review'] = test_data['review'].str.replace(r'[^\\w\\s]+', '')\n",
    "        test_data['review'] = test_data['review'].str.lower()\n",
    "\n",
    "\n",
    "        # Will take the 10000 most frequent words\n",
    "        vectorizer = CountVectorizer(max_features=10000)\n",
    "        vectorizer.fit(train_data['review'])\n",
    "        train_vectors = vectorizer.transform(train_data['review'])\n",
    "        test_vectors = vectorizer.transform(test_data['review'])\n",
    "        val_vectors = vectorizer.transform(val_data['review'])\n",
    "        words = vectorizer.get_feature_names()\n",
    "        train_matrix = np.asarray(train_vectors)\n",
    "        frequency_vector = np.asarray(train_vectors.sum(axis=0)).reshape(10000,)\n",
    "        frequency_list = frequency_vector.tolist()\n",
    "\n",
    "        index_list = range(10000)\n",
    "        frequency_list, index_list, words = zip(*sorted(zip(frequency_list, index_list, words), reverse=True))\n",
    "\n",
    "        # Get the vocabulary. \n",
    "        vocabulary = vectorizer.vocabulary_\n",
    "\n",
    "        with open(\"imdb-vocab.txt\",'w') as vocab_file:\n",
    "            for i in range(10000):\n",
    "                vocab_file.write(\"{0:<12}\\t{1:>5}\\t{2:>8}\\n\".format(words[i], index_list[i], frequency_list[i] ))\n",
    "\n",
    "        # Build train file\n",
    "        nb_examples_train = 15000\n",
    "        nb_examples_val = 10000\n",
    "        nb_examples_test = 25000\n",
    "        nb_features = 10000\n",
    "        # Set up BBoW with 1 if example has word in index, 0 if not\n",
    "        BBOW_trainx = np.zeros((nb_examples_train, nb_features))\n",
    "        BBOW_trainy = np.zeros((nb_examples_train,))\n",
    "        # Set up FBoW with word_count/total_count if example has word in index, 0 if not\n",
    "        FBOW_trainx = np.zeros((nb_examples_train, nb_features))\n",
    "        FBOW_trainy = np.zeros((nb_examples_train,))\n",
    "        example = 0\n",
    "        with open(\"imdb-train.txt\",'w') as train_file:\n",
    "            for review in train_data['review']:\n",
    "                occurances = 0\n",
    "                words = review.split()\n",
    "                paragraph = \"\"\n",
    "                for word in words:\n",
    "                    index = vocabulary.get(word)\n",
    "                    if index is not None:\n",
    "                        paragraph += str(index)+ \" \"\n",
    "                        BBOW_trainx[example, index] = 1\n",
    "                        FBOW_trainx[example, index] += 1\n",
    "                        occurances += 1\n",
    "                BBOW_trainy[example] = train_data['score'][example]\n",
    "                FBOW_trainy[example] = train_data['score'][example]\n",
    "                train_file.write(\"{}\\t{}\\n\".format(paragraph, train_data['score'][example]))\n",
    "                if occurances != 0:\n",
    "                    FBOW_trainx[example] /= occurances\n",
    "                example += 1\n",
    "        np.savetxt(\"inputs/imdb-train-bbow_x.txt\", BBOW_trainx, delimiter=\",\", fmt='%d')\n",
    "        np.savetxt(\"inputs/imdb-train-bbow_y.txt\", BBOW_trainy, delimiter=\",\", fmt='%d')\n",
    "        np.savetxt(\"inputs/imdb-train-fbow_x.txt\", FBOW_trainx, delimiter=\",\", fmt='%1.5f')\n",
    "        np.savetxt(\"inputs/imdb-train-fbow_y.txt\", FBOW_trainy, delimiter=\",\", fmt='%d')\n",
    "\n",
    "        # Build validation file\n",
    "        BBOW_valx = np.zeros((nb_examples_val, nb_features))\n",
    "        BBOW_valy = np.zeros((nb_examples_val,))\n",
    "        FBOW_valx = np.zeros((nb_examples_val, nb_features))\n",
    "        FBOW_valy = np.zeros((nb_examples_val,))\n",
    "        example = 0\n",
    "        with open(\"imdb-val.txt\",'w') as val_file:\n",
    "            for review in val_data['review']:\n",
    "                occurances = 0\n",
    "                words = review.split()\n",
    "                paragraph = \"\"\n",
    "                for word in words:\n",
    "                    index = vocabulary.get(word)           \n",
    "                    if index is not None:\n",
    "                        paragraph += str(index)+ \" \"\n",
    "                        BBOW_valx[example, index] = 1\n",
    "                        FBOW_valx[example, index] += 1\n",
    "                        occurances += 1\n",
    "                BBOW_valy[example] = val_data['score'][example]\n",
    "                FBOW_valy[example] = val_data['score'][example]\n",
    "                val_file.write(\"{}\\t{}\\n\".format(paragraph, val_data['score'][example]))\n",
    "                if occurances != 0:\n",
    "                    FBOW_valx[example] /= occurances\n",
    "                example += 1\n",
    "        np.savetxt(\"inputs/imdb-val-bbow_x.txt\", BBOW_valx, delimiter=\",\", fmt='%d')\n",
    "        np.savetxt(\"inputs/imdb-val-bbow_y.txt\", BBOW_valy, delimiter=\",\", fmt='%d')\n",
    "        np.savetxt(\"inputs/imdb-val-fbow_x.txt\", FBOW_valx, delimiter=\",\", fmt='%1.5f')\n",
    "        np.savetxt(\"inputs/imdb-val-fbow_y.txt\", FBOW_valy, delimiter=\",\", fmt='%d')\n",
    "\n",
    "        # Build test file\n",
    "        BBOW_testx = np.zeros((nb_examples_test, nb_features))\n",
    "        BBOW_testy = np.zeros((nb_examples_test,))\n",
    "        FBOW_testx = np.zeros((nb_examples_test, nb_features))\n",
    "        FBOW_testy = np.zeros((nb_examples_test,))\n",
    "        example = 0\n",
    "        with open(\"imdb-test.txt\",'w') as test_file:\n",
    "            for review in test_data['review']:\n",
    "                occurances = 0\n",
    "                words = review.split()\n",
    "                paragraph = \"\"\n",
    "                for word in words:\n",
    "                    index = vocabulary.get(word)           \n",
    "                    if index is not None:\n",
    "                        paragraph += str(index)+ \" \"\n",
    "                        BBOW_testx[example, index] = 1\n",
    "                        FBOW_testx[example, index] += 1\n",
    "                        occurances += 1\n",
    "                BBOW_testy[example] = test_data['score'][example]\n",
    "                FBOW_testy[example] = test_data['score'][example]\n",
    "                test_file.write(\"{}\\t{}\\n\".format(paragraph, test_data['score'][example]))\n",
    "                if occurances != 0:\n",
    "                    FBOW_testx[example] /= occurances\n",
    "                example += 1\n",
    "        np.savetxt(\"inputs/imdb-test-bbow_x.txt\", BBOW_testx, delimiter=\",\", fmt='%d')\n",
    "        np.savetxt(\"inputs/imdb-test-bbow_y.txt\", BBOW_testy, delimiter=\",\", fmt='%d')\n",
    "        np.savetxt(\"inputs/imdb-test-fbow_x.txt\", FBOW_testx, delimiter=\",\", fmt='%1.5f')\n",
    "        np.savetxt(\"inputs/imdb-test-fbow_y.txt\", FBOW_testy, delimiter=\",\", fmt='%d')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input files already exist\n",
      "Loading training set...\n",
      "Loading validation set...\n",
      "Loading test set...\n"
     ]
    }
   ],
   "source": [
    "# Load up arrays\n",
    "imdb_train_bbow_x = 'inputs/imdb-train-bbow_x.txt'\n",
    "imdb_train_bbow_y ='inputs/imdb-train-bbow_y.txt'\n",
    "imdb_val_bbow_x ='inputs/imdb-val-bbow_x.txt'\n",
    "imdb_val_bbow_y ='inputs/imdb-val-bbow_y.txt'\n",
    "imdb_test_bbow_x ='inputs/imdb-test-bbow_x.txt'\n",
    "imdb_test_bbow_y ='inputs/imdb-test-bbow_y.txt'\n",
    "if os.path.isfile(imdb_train_bbow_x) and os.path.isfile(imdb_val_bbow_x) and os.path.isfile(imdb_test_bbow_x):\n",
    "    print(\"Input files already exist\")\n",
    "    print(\"Loading training set...\")\n",
    "    train_x = np.loadtxt(imdb_train_bbow_x, delimiter=',')\n",
    "    train_y = np.loadtxt(imdb_train_bbow_y, delimiter=',')\n",
    "    print(\"Loading validation set...\")\n",
    "    val_x = np.loadtxt(imdb_val_bbow_x, delimiter=',')\n",
    "    val_y = np.loadtxt(imdb_val_bbow_y, delimiter=',')\n",
    "    print(\"Loading test set...\")\n",
    "    test_x = np.loadtxt(imdb_test_bbow_x, delimiter=',')\n",
    "    test_y = np.loadtxt(imdb_test_bbow_y, delimiter=',')\n",
    "else:\n",
    "    print(\"--------Creating input files (might take a while)---------------\")\n",
    "    if not os.path.exists('inputs'):\n",
    "        os.makedirs('inputs')\n",
    "    imdb_load()\n",
    "    print(\"Loading training set...\")\n",
    "    train_x = np.loadtxt(imdb_train_bbow_x, delimiter=',')\n",
    "    train_y = np.loadtxt(imdb_train_bbow_y, delimiter=',')\n",
    "    print(\"Loading validation set...\")\n",
    "    val_x = np.loadtxt(imdb_val_bbow_x, delimiter=',')\n",
    "    val_y = np.loadtxt(imdb_val_bbow_y, delimiter=',')\n",
    "    print(\"Loading test set...\")\n",
    "    test_x = np.loadtxt(imdb_test_bbow_x, delimiter=',')\n",
    "    test_y = np.loadtxt(imdb_test_bbow_y, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_classifier(classifier): \n",
    "    train_yh =classifier.predict(train_x)\n",
    "    val_yh = classifier.predict(val_x)\n",
    "    test_yh = classifier.predict(test_x)\n",
    "    \n",
    "    # Check performance\n",
    "    train_ascore = accuracy_score(train_y, train_yh)\n",
    "    val_ascore = accuracy_score(val_y, val_yh)\n",
    "    test_ascore = accuracy_score(test_y, test_yh)\n",
    "   \n",
    "    val_f1 = f1_score(val_y, val_yh, average='micro')\n",
    "    train_f1 = f1_score(train_y, train_yh, average='micro')\n",
    "    test_f1 = f1_score(test_y, test_yh, average='micro')\n",
    "    \n",
    "    print(\"Train F1: {}\".format(train_f1))\n",
    "    print(\"Val F1: {}\".format(val_f1))\n",
    "    print(\"Test F1: {}\".format(test_f1))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter Tuning\n",
    "---------------------------------\n",
    "We'll try a grid search over several parameters. The values are refined \n",
    "in the neighbourhood of where performance is best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------Random Classifier ----------------\n",
      "Train F1: 0.49893333333333334\n",
      "Val F1: 0.4954\n",
      "Test F1: 0.50564\n",
      "\n",
      "\n",
      "-------- Naive Bayes Classifier ----------------\n",
      "Hyperparameter tuning...\n",
      "Optimum hyper-parameters:  {'alpha': 0.14}\n",
      "\n",
      "Evaluating...\n",
      "Train F1: 0.8722\n",
      "Val F1: 0.8414\n",
      "Test F1: 0.8326\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('--------Random Classifier ----------------')\n",
    "random = DummyClassifier(strategy='uniform', random_state=1) \n",
    "random.fit(train_x, train_y)\n",
    "eval_classifier(random)\n",
    "\n",
    "print('-------- Naive Bayes Classifier ----------------')\n",
    "# Here we setup a training validation combined array for cross \n",
    "# validation\n",
    "cv_x = np.append(train_x, val_x, axis = 0)\n",
    "cv_y = np.append(train_y, val_y, axis = 0)\n",
    "\n",
    "print('Hyperparameter tuning...')\n",
    "# Tuning the smoothing parameter 'alpha' of the error term. We locate it on the\n",
    "# range (0.1, 0.2) and find the optimum at 0.14\n",
    "bayes = BernoulliNB()\n",
    "alpha_space = [0.13, 0.14, 0.15]  \n",
    "alpha_list = [a for a in alpha_space]\n",
    "parameters={'alpha' : alpha_list}\n",
    "classifier = GridSearchCV(bayes, parameters, cv = 5, scoring='f1_micro')\n",
    "classifier.fit(cv_x, cv_y)\n",
    "print(\"Optimum hyper-parameters: \", classifier.best_params_)\n",
    "\n",
    "# Now fit model with optimum hyperparameters\n",
    "print('\\nEvaluating...')\n",
    "opt_alpha = classifier.best_params_['alpha']\n",
    "classifier = BernoulliNB(alpha=opt_alpha)\n",
    "classifier.fit(train_x, train_y)\n",
    "eval_classifier(classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------- Decision Tree Classifier ----------------\n",
      "Hyperparameter tuning...\n",
      "{'max_depth': 18}\n",
      "\n",
      "Evaluating...\n",
      "Train F1: 0.8573333333333333\n",
      "Val F1: 0.7237999999999999\n",
      "Test F1: 0.7264\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('\\n-------- Decision Tree Classifier ----------------')\n",
    "print('Hyperparameter tuning...')\n",
    "# Tuning the depth (other decision tree hyperparameters not seen in class\n",
    "# will not be considered here)\n",
    "max_depths = [17, 18, 19] \n",
    "\n",
    "tree = DecisionTreeClassifier(random_state=1)\n",
    "parameters={'max_depth' : max_depths}\n",
    "classifier = GridSearchCV(tree, parameters, cv = 5, scoring='f1_micro')\n",
    "classifier.fit(cv_x, cv_y)\n",
    "print(classifier.best_params_)\n",
    "\n",
    "# Now train on optimum hyperparameters\n",
    "print('\\nEvaluating...')\n",
    "opt_parameters = classifier.best_params_\n",
    "classifier = DecisionTreeClassifier(random_state=1,max_depth=opt_parameters['max_depth'])\n",
    "                                  \n",
    "classifier.fit(train_x, train_y)\n",
    "eval_classifier(classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Linear SVM Classifier ----------------\n",
      "Hyperparameter tuning...\n",
      "Optimum hyper-parameters:  {'C': 0.006}\n",
      "\n",
      "Evaluating...\n",
      "Train F1: 0.9504\n",
      "Val F1: 0.8768\n",
      "Test F1: 0.87132\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('-------- Linear SVM Classifier ----------------')\n",
    "print('Hyperparameter tuning...')\n",
    "# Tuning the penalty parameter 'C' of the error term. We locate it on the\n",
    "# range (0.005, 0.008) and find the optimum at 0.006\n",
    "parameters={'C': [0.005, 0.006, 0.007]} \n",
    "\n",
    "svm_clf = svm.LinearSVC(random_state=1, max_iter=5000)\n",
    "classifier = GridSearchCV(svm_clf, parameters, cv = 5, scoring='f1_micro')\n",
    "classifier.fit(cv_x, cv_y)\n",
    "print(\"Optimum hyper-parameters: \", classifier.best_params_)\n",
    "\n",
    "# Now train on optimum hyperparameters\n",
    "opt_C = classifier.best_params_['C']\n",
    "print('\\nEvaluating...')\n",
    "svm_classifier = svm.LinearSVC(random_state=1, C=opt_C, max_iter=5000)\n",
    "svm_classifier.fit(train_x, train_y)\n",
    "eval_classifier(svm_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
